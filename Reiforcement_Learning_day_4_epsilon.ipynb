{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKSnAC5G5LAP",
        "outputId": "3a4e2edd-c892-4aba-a39f-e6edbfbbe424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8, 0.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/JKCooper2/gym-bandits.git > /dev/null 2>&1\n",
        "!pip install /content/gym-bandits/. > /dev/null 2>&1\n",
        "\n",
        "\n",
        "import gym\n",
        "import gym_bandits\n",
        "env=gym.make('BanditTwoArmedHighLowFixed-v0')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.p_dist)\n",
        "# We can observe that with arm 1 we win the game with 80% probability and iwth arm 2 we win the game with 20% probability. Here, the best arm is arm 1, as with the arm 1 we win\n",
        "# with 80% probability, Now let's see the best arm and find it using the epsilon-greedy method"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoV3uFVB5Neh",
        "outputId": "1afe5f18-6c87-40f4-f3a6-10f68c716df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8, 0.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the variables\n",
        "# count - used for storing the number of times a arm is pullled\n",
        "import numpy as np\n",
        "count=np.zeros(2)\n",
        "\n",
        "# initialization process"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VSEhDsU5Ra-",
        "outputId": "14422ff2-f722-4282-b330-9495482ecc82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sum_rewards -> storing the sum of rewards for each arm\n",
        "sum_rewards = np.zeros(2)\n",
        "\n",
        "# initialization process"
      ],
      "metadata": {
        "id": "hjh53RiL5Wtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# q -> storing the aveage rewards of each arm\n",
        "q =np.zeros(2)\n",
        "\n",
        "# initialization process"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_hDPw926Z41",
        "outputId": "479b0ab7-58d5-45e6-d061-26f9a72c6aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# num_rounds - > number of rounds ( iterations ) :\n",
        "num_rounds = 100\n",
        "\n",
        "# initialization process"
      ],
      "metadata": {
        "id": "65KJBjZ07AzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the epsilon greedy method\n",
        "# epsilon value is 0.5\n",
        "\n",
        "def epsilon_greedy(epsilon):\n",
        "  if np.random.uniform(0,1) < epsilon :\n",
        "    return env.action_space.sample() # Exploration\n",
        "  else:\n",
        "    return np.argmax(q) # Exploitation\n",
        "\n",
        "# Defining epsilon_greedy function: You're defining a function named epsilon_greedy that takes one parameter, epsilon.\n",
        "# This function will be responsible for making a decision on whether to explore or exploit based on the epsilon value provided.\n",
        "\n",
        "# Exploration (random action): If a random number generated between 0 and 1 is less than epsilon, the agent chooses to explore.\n",
        "# In this case, the function returns env.action_space.sample(), which generates a random action from the environment's action space.\n",
        "# This random action allows the agent to explore different arms of the bandit.\n",
        "\n",
        "# Exploitation (selecting the best action): If the random number is not less than epsilon, the agent chooses to exploit.\n",
        "# In this case, the function returns np.argmax(q), which selects the arm with the highest average reward (based on the q array) for exploitation.\n",
        "# The np.argmax(q) function returns the index of the arm with the highest average reward.\n",
        "\n",
        "# The epsilon-greedy method is a fundamental strategy in reinforcement learning.\n",
        "# It balances the exploration of unknown options (exploration) with the exploitation of currently known best options (exploitation).\n",
        "# The epsilon parameter controls the trade-off between exploration and exploitation, with higher values of epsilon encouraging more exploration and lower values favoring exploitation."
      ],
      "metadata": {
        "id": "wTh4voiL7QEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_rounds):\n",
        "  # select the arm based on the epsilon-greedy method\n",
        "  arm = epsilon_greedy(0.5)\n",
        "  # call epsilon-grredy agent(policy)\n",
        "\n",
        "  env.reset()\n",
        "  # pull the arm and store  the reward and next state\n",
        "\n",
        "  next_state,reward,done,info=env.step(arm)\n",
        "\n",
        "  # increment the count of the arm by 1\n",
        "  count[arm] += 1\n",
        "\n",
        "  # updatethe sum of rewards of the arm\n",
        "  sum_rewards[arm] += reward\n",
        "\n",
        "  # update the average reward of the arm\n",
        "  q[arm] = sum_rewards[arm]/count[arm]\n",
        "\n",
        "# Iteration Loop: The loop iterates through num_rounds (100 in this example) to simulate interactions between the agent and the environment.\n",
        "\n",
        "# Arm Selection: The agent selects an arm using the epsilon-greedy method, passing 0.5 as the epsilon value.\n",
        "# This means there's a 50% chance of exploration (random arm selection) and a 50% chance of exploitation (selecting the best-known arm).\n",
        "\n",
        "# Environment Reset: The environment is reset to its initial state at the beginning of each round. This ensures that the agent starts fresh with each iteration.\n",
        "\n",
        "# Arm Pulling and Reward: The agent pulls the selected arm in the environment using the env.step(arm) function.\n",
        "# It receives the next state, reward, a flag indicating if the episode is done, and additional information.\n",
        "\n",
        "# Updating Count: The count array keeps track of how many times each arm has been pulled. After pulling an arm, the count for that arm is incremented by 1.\n",
        "\n",
        "# Updating Sum of Rewards: The sum_rewards array accumulates the rewards obtained from pulling each arm. The reward obtained from the current round is added to the sum for the selected arm.\n",
        "\n",
        "# Updating Average Reward: The q array stores the average reward for each arm. It's updated by dividing the accumulated sum of rewards by the count of pulls for the selected arm.\n",
        "\n",
        "# By repeatedly performing these steps for the specified number of rounds, the agent learns and refines its strategy to maximize its cumulative reward over time.\n",
        "# This iterative process helps the agent find the arm with the highest expected reward and exploit it more frequently.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmOASMyR7Rcj",
        "outputId": "3c114939-d50c-4572-d008-a10b8937a53f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Id-hvVUBIcM",
        "outputId": "50592c4a-af70-4396-e0a9-545cf47e1b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.70238095 0.16666667]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4TWr6uMpBexM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}