{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKSnAC5G5LAP",
        "outputId": "4736a7c3-7237-4432-eb11-7f2db7976531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/JKCooper2/gym-bandits.git > /dev/null 2>&1\n",
        "!pip install /content/gym-bandits/. > /dev/null 2>&1\n",
        "\n",
        "\n",
        "import gym\n",
        "import gym_bandits\n",
        "env=gym.make('BanditTwoArmedHighLowFixed-v0')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.p_dist)\n",
        "# We can observe that with arm 1 we win the game with 80% probability and iwth arm 2 we win the game with 20% probability. Here, the best arm is arm 1, as with the arm 1 we win\n",
        "# with 80% probability, Now let's see the best arm and find it using the epsilon-greedy method"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoV3uFVB5Neh",
        "outputId": "644782e9-e6af-43da-a235-a22d3892f798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8, 0.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the variables\n",
        "# count - used for storing the number of times a arm is pullled\n",
        "import numpy as np\n",
        "count=np.zeros(2)\n",
        "\n",
        "# initialization process"
      ],
      "metadata": {
        "id": "4VSEhDsU5Ra-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sum_rewards -> storing the sum of rewards for each arm\n",
        "sum_rewards = np.zeros(2)\n",
        "\n",
        "# initialization process"
      ],
      "metadata": {
        "id": "hjh53RiL5Wtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# q -> storing the aveage rewards of each arm\n",
        "q =np.zeros(2)\n",
        "\n",
        "# initialization process"
      ],
      "metadata": {
        "id": "7_hDPw926Z41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# num_rounds - > number of rounds ( iterations ) :\n",
        "num_rounds = 100\n",
        "\n",
        "# initialization process"
      ],
      "metadata": {
        "id": "65KJBjZ07AzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(T):\n",
        "    # Compute the probability of each arm based on the softmax equation\n",
        "    # Temperature T controls the exploration-exploitation trade-off\n",
        "\n",
        "    # Calculate the denominator by summing the exponentials of the Q-values\n",
        "    denom = sum([np.exp(i/T) for i in q])\n",
        "\n",
        "    # Calculate the probability distribution using the softmax formula\n",
        "    probs = ([np.exp(i/T) / denom for i in q])\n",
        "\n",
        "    # Select the arm based on the computed probability distribution\n",
        "    # The np.random.choice() function samples an arm using the computed probabilities\n",
        "    arm = np.random.choice(env.action_space.n, p=probs)\n",
        "\n",
        "    # Return the selected arm based on the computed probability distribution\n",
        "    return arm\n",
        "\n",
        "# Function Definition: This is the definition of the softmax function that takes a temperature parameter T.\n",
        "\n",
        "# Calculating Denominator: The denominator is computed by summing the exponentials of the Q-values (Q). The Q-values represent the expected rewards for each arm.\n",
        "\n",
        "# Calculating Probability Distribution: The probability distribution is calculated using the softmax formula.\n",
        "# The Q-values are divided by the denominator to obtain the probabilities.\n",
        "\n",
        "# Selecting Arm: The np.random.choice() function is used to select an arm from the action space based on the computed probability distribution.\n",
        "# The p parameter specifies the probabilities for each arm.\n",
        "\n",
        "# Returning Selected Arm: The selected arm is returned as the output of the function.\n",
        "\n",
        "# In simple words, the softmax function takes a temperature parameter as input.\n",
        "# It uses this parameter to control the exploration-exploitation trade-off:\n",
        "# higher values of T encourage more exploration, while lower values encourage more exploitation of known high-reward arms.\n",
        "# The function calculates a probability distribution based on the Q-values, where higher Q-values lead to higher probabilities of selecting the corresponding arm.\n",
        "# The softmax function then randomly selects an arm using these computed probabilities and returns the selected arm."
      ],
      "metadata": {
        "id": "wTh4voiL7QEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3f03038-b85d-4994-d65c-b9ce8f42912e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T=15\n",
        "for i in range(num_rounds):\n",
        "  # Compute the probability of each arm based on the softmax equation\n",
        "    # Temperature T controls the exploration-exploitation trade-off\n",
        "  arm = softmax(T)\n",
        "\n",
        "  env.reset()\n",
        "  # pull the arm and store  the reward and next state\n",
        "\n",
        "  next_state,reward,done,info=env.step(arm)\n",
        "\n",
        "  # increment the count of the arm by 1\n",
        "  count[arm] += 1\n",
        "\n",
        "  # updatethe sum of rewards of the arm\n",
        "  sum_rewards[arm] += reward\n",
        "\n",
        "  # update the average reward of the arm\n",
        "  q[arm] = sum_rewards[arm]/count[arm]\n",
        "\n",
        "  # reduce the temperature\n",
        "  T=T*0.99\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmOASMyR7Rcj",
        "outputId": "330e071f-a225-4126-dd30-6d2f6ed74eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Id-hvVUBIcM",
        "outputId": "f6350575-8da7-4569-95a4-14445fe6c40b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.84745763 0.26829268]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(q)"
      ],
      "metadata": {
        "id": "4TWr6uMpBexM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f82301e-d471-4a42-9d80-5bf5582b749e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g7c0oFRiELkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jRVXjJy-FDHT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}